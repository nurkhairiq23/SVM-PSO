{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbd88b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>love</td>\n",
       "      <td>Tahukah kamu, bahwa saat itu papa memejamkan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>fear</td>\n",
       "      <td>Sulitnya menetapkan Calon Wapresnya Jokowi di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>anger</td>\n",
       "      <td>5. masa depannya nggak jelas. lha iya, gimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>happy</td>\n",
       "      <td>[USERNAME] dulu beneran ada mahasiswa Teknik U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Ya Allah, hanya Engkau yang mengetahui rasa sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                              tweet\n",
       "0       anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
       "1       anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
       "2       happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
       "3       anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
       "4       happy  Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
       "...       ...                                                ...\n",
       "4396     love  Tahukah kamu, bahwa saat itu papa memejamkan m...\n",
       "4397     fear  Sulitnya menetapkan Calon Wapresnya Jokowi di ...\n",
       "4398    anger  5. masa depannya nggak jelas. lha iya, gimana ...\n",
       "4399    happy  [USERNAME] dulu beneran ada mahasiswa Teknik U...\n",
       "4400  sadness  Ya Allah, hanya Engkau yang mengetahui rasa sa...\n",
       "\n",
       "[4401 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Dataset/Twitter_Emotion_Dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45570be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case Folding\n",
    "data['tweet_case_folding'] = data['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad25fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleansing\n",
    "import string\n",
    "import re #regex Library\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    #Remove kata istilah dari dataset ([username], [url], [sensitive-no]) \n",
    "    text = text.replace('[username]',\" \").replace('[url]',\" \").replace('[sensitive-no]',\" \")\n",
    "\n",
    "    #Remove tab, new Line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    \n",
    "    #Remove non ASCII (emoticon, chinese word, etc) \n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "\n",
    "    #Remove mention, Link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "\n",
    "    #Remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "data['tweet_cleansing'] = data['tweet_case_folding'].apply(remove_tweet_special)\n",
    "\n",
    "#Remove number\n",
    "def remove_number (text):\n",
    "    return re.sub(r\"\\d+\", \" \", text)\n",
    "data['tweet_cleansing'] = data['tweet_cleansing'].apply(remove_number)\n",
    "\n",
    "#Remove punctuation\n",
    "def remove_punctuation(text): \n",
    "    return text.translate(str.maketrans(\" \",\" \",string.punctuation))\n",
    "data['tweet_cleansing'] = data['tweet_cleansing'].apply(remove_punctuation)\n",
    "\n",
    "#Remove whitespace Loading & trailing \n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "data['tweet_cleansing'] = data['tweet_cleansing'].apply(remove_whitespace_LT)\n",
    "\n",
    "#Remove multiple whitespace into single whitespace \n",
    "def remove_whitespace_multiple (text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "data['tweet_cleansing'] = data['tweet_cleansing'].apply(remove_whitespace_multiple)\n",
    "\n",
    "#Remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "data['tweet_cleansing'] = data['tweet_cleansing'].apply(remove_singl_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c943fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet_cleansing'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e104c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "normalizad_word = pd.read_excel(\"Dataset/kamus_singkatan2.xlsx\")\n",
    "\n",
    "#Tranform Dataframe to Dictionary\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict: \n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "data['tweet_normalized'] = data['tweet_tokenized'].apply(normalized_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7339d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c16d1814510487aa4511f322603b27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Stemming\n",
    "#!pip install PySastrawi\n",
    "#!pip install swifter\n",
    "\n",
    "#import sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory \n",
    "import swifter\n",
    "\n",
    "#Create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#stemming\n",
    "def stemmed_wrapper(term): \n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "for document in data['tweet_normalized']:\n",
    "    for term in document: \n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term) \n",
    "    #print(term, \":\", term_dict[term])  \n",
    "\n",
    "# apply stemmed term to dataframe \n",
    "def get_stemmed_term(document): \n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data['tweet_stemming'] = data['tweet_normalized'].swifter.apply(get_stemmed_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c520d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "#manualy add stopword ---\n",
    "\n",
    "# append additional stopword \n",
    "list_stopwords.extend(['yg', 'dg', 'dgn', 'rt', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bikin', \n",
    "                       'bilang', 'birthday', 'doang', 'dong', 'cari', 'itu', 'krn', 'karna', 'nya', \n",
    "                       'nih', 'sih', 'udh', 'sampai', 'kenapa', 'duluan', 'ada', 'abis', 'ugh', \n",
    "                       'pengen', 'si', 'tau', 'tuh', 'utk', 'ya', 'trs', 'sm', 'padahal', 'lagi', \n",
    "                       'dpt', 'dapat', 'dapet', 'ken', 'mlu', 'jd', 'sdh', 'aja', 'n', 't', 'pas', \n",
    "                       'yang', 'apa', 'banyak', 'buat', 'pls', 'mulu', 'cari', 'nyg', 'hehe', \n",
    "                       'pen', 'u', 'pap', 'loh', 'emg', 'buat', 'sdg', 'pada', 'pda', 'allah', \n",
    "                       'ydh', 'yaudah','&amp', 'banget', 'yah', 'lha', 'lho'])\n",
    "\n",
    "#read stopword from txt file\n",
    "#txt_stopword = pd.read_csv(\"Dataset/stopwords.txt\", names = [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword \n",
    "#list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# convert list to dictionary \n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "\n",
    "def stopwords_removal(words): \n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "data['tweet_stopwords'] = data['tweet_stemming'].apply(stopwords_removal) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a0a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_token(word):\n",
    "    word = np.array(word)\n",
    "    word = ' '.join(word)\n",
    "    return word\n",
    "data['clean_data'] = data['tweet_stopwords'].apply(fit_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2724758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "      <td>jalan jatibarupolisi gertak gubernur emangny p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "      <td>cewek kayak rasain sibuk jaga rasain sakit hai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "      <td>kepingin gudeg mbarek bu hj amad foto google s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "      <td>jalan jatibarubagian wilayah tn abangpengatura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "      <td>sharing alam kemarin jam batalin tiket stasiun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0  anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...   \n",
       "1  anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...   \n",
       "2  happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...   \n",
       "3  anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...   \n",
       "4  happy  Sharing pengalaman aja, kemarin jam 18.00 bata...   \n",
       "\n",
       "                                          clean_data  \n",
       "0  jalan jatibarupolisi gertak gubernur emangny p...  \n",
       "1  cewek kayak rasain sibuk jaga rasain sakit hai...  \n",
       "2  kepingin gudeg mbarek bu hj amad foto google s...  \n",
       "3  jalan jatibarubagian wilayah tn abangpengatura...  \n",
       "4  sharing alam kemarin jam batalin tiket stasiun...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = {'tweet': data['tweet'], \n",
    "           'case_folding': data['tweet_case_folding'], \n",
    "           'cleansing': data['tweet_cleansing'], \n",
    "           'tokenization': data['tweet_tokenized'], \n",
    "           'normalization': data['tweet_normalized'], \n",
    "           'stemming': data['tweet_stemming'], \n",
    "           'stopwords': data['tweet_stopwords']}\n",
    "dataset2 = {'label': data['label'],\n",
    "            'tweet': data['tweet'],\n",
    "           'clean_data': data['clean_data']}\n",
    "df = pd.DataFrame(dataset)\n",
    "df2 = pd.DataFrame(dataset2)\n",
    "df.head()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d095c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Dataset/Hasil Preprocessing Data.xlsx', index=False)\n",
    "df2.to_excel('Dataset/Clean Dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea47b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
